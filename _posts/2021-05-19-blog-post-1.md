---
title: 'Reinforcement Learning: Part 1'
date: 2021-05-19
permalink: /posts/2021/05/rl-1/
categories:
  - Computer Science
tags:
  - Reinforcement Learning
---

A discussion what reinforcement learning (RL) is and what its key elements are. Updating...

# Reinforcement Learning

The problem RL addresses is how an **agent** can maximize its expected cumulative **reward** in a complex and uncertain **environment**. The interaction between agent and environment involves

- agent obtaining the current **state** from the environment;
- agent outputting an **action** (aka a decision) based on the states;
- environment returning the next state and reward which directly results from the previous action taken.

**Remark** An action space can be either discrete or continuous. 

Now, we shall discuss the idea of *Sequential Decision Making* behind RL.

## Sequential Decision Making

In aim of maximizing the cumulative reward, normally speaking, a good action taken should have a long-term impact, which means that the reward obtained after taking an action will in effect be **delayed**. Therefore, the design of a RL algorithm must carefully determine the trade-off between short-term and long-term reward. 

When interacting with environment, we define history  as a sequence of observations, actions and rewards:
$$
H_t = O_1, R_1, A_1, \dots, A_{t-1}, O_t, R_t
$$
Since agent will act based on the history, the state of environment can be viewed as a function of the history, $S^e_t = f^e(H_t)$. Since the state the agent perceive may not be the same as the environment per se, we write $S^a_t = f^a(H_t)$ for the agent. An environment is *fully observed* when agent can observe the full state of the environment, in which case the problem is usually modelled as a Markov decision process (MDP),  i.e., $O_t = S^e_t = S^a_t$.

In contrast, when agent is not able to observe the full state, i.e., environment is partially observed, the problem is modelled as a partially observable MDP (POMDP), which is a generalize of the traditional one. It still possesses Markov property, but can only get hold of the partial observation $o$ instead of the state $s$. For instance, a robot only have limited information of the environment from, say, sensors. POMDP can be described by $(S, A, T, R, \Omega, O, \gamma)$, with the elements representing the state space (implicit variable), action space, state transition probability $T(s'|s, a)$, reward function, observation probability $\Omega(o|s, a)$, observation space and discount factor respectively.

## Major Components of Agent

Namely, *policy function*, *value function* and *model*.

### Policy Function

Policy is essentially the behavior model of an agent, in the form of a function transforming the inputted state to an action. There are 2 types of policy:

- **Stochastic policy** $\pi(a|s) = P[At=a|S_t=s]$, action determined by sampling from the probability distribution
- **Deterministic policy** $a*=\arg \max \pi(a|s)$

The former one is commonly used in RL because

- Randomness $\to$ more exploration
- A variety of actions can be taken for the same policy under the same state. This is particularly useful in the case of a multi-agent environment, where reacting consistently will make an agent's policy predictable to the others.

### Value Function



Intuitively, we say the difficulty of a environment depends on how "sparse" the rewards are across a round of a game.  

## Comparison with Supervised Learning

Supervised learning methods are based on the hypotheses:

- Input data are independent, i.e., they are i.i.d. (independent & identically distributed). 
- A label is provided with each datum.

Both are necessary to train a classifier (e.g., a neural network) via backpropagation of error between the true value (i.e., the label) and the predicted one.

Apparently, none of the hypotheses can apply to the problem described above, for its inputs are time-series data, resulting in a delayed reward; no "correct value" can be fed into the learner, i.e., agent has to explore via trial-and-error to discover the best action to take under a given state. Hence, one needs to strike a balance between *exploration* and *exploitation*, where the former takes some "risky" moves and the later maintains the old decision-making policy to obtain the same reward. 

## Training Procedure

Keeping the idea of **delayed** reward in mind, the series of actions taken will only be judged of its contribution in obtaining the reward after a game is ended. So reinforcement learning in short is to sample actions until game is over, then penalize each action.

Let's now introduce some definitions:

Rollout
: An execution of a policy from the current state when there is some uncertainty about the next state or outcome, i.e., one simulation from the current state

Trajectory

: A rollout sequence of states and actions, $\tau = (s_0, a_0, s_1, a_1, \dots)$

Episode / Trial

: A round of game

## Remark

Same as deep learning to traditional machine learning, deep RL adopts neural networks to achieve end-to-end training, without the need of feature engineering. It can optimize feature extraction and evaluation (or decision-making policy) simultaneously, resulting in a stronger policy network.

# Sequential Decision Making

