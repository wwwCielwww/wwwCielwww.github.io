---
title: 'Reinforcement Learning: Notes 1'
date: 2021-05-19
permalink: /posts/2021/05/rl-1/
categories:
  - Computer Science
tags:
  - Reinforcement Learning
---

A high-level overview of problems that reinforcement learning can address, key ideas which it rely upon, its major components, as well as the differences to supervised learning

I hope the post can make you think that the whole idea of RL might actually work. We'll talk about the details in later parts of the series. ðŸ’™

# Reinforcement Learning

The problem RL addresses is how an **agent** can maximize its expected cumulative **reward** in a complex and uncertain **environment**. The interaction between agent and environment involves

- agent obtaining the current **state** from the environment;
- agent outputting an **action** (aka a decision) based on the states;
- environment returning the next state and reward which directly results from the previous action taken.

**Remark** An action space can be either discrete or continuous. 

Now, we shall discuss the idea of *Sequential Decision Making* behind RL.

## Sequential Decision Making

In aim of maximizing the cumulative reward, normally speaking, a good action taken should have a long-term impact, which means that the reward obtained after taking an action will in effect be **delayed**. Therefore, the design of a RL algorithm must take the trade-off between short-term and long-term reward into consideration. 

When interacting with environment, we define history  as a sequence of observations, actions and rewardsã€‚
$$
H_t = O_1, R_1, A_1, \dots, A_{t-1}, O_t, R_t
$$
Since agent will act based on the history, the state of environment can be viewed as a function of the history, $S^e_t = f^e(H_t)$. Since the state the agent perceive may not be the same as the environment per se, we write $S^a_t = f^a(H_t)$ for the agent. An environment is *fully observed* when agent can observe the full state of the environment, in which case the problem is usually modelled as a Markov decision process (MDP),  i.e., $O_t = S^e_t = S^a_t$.

In contrast, when agent is not able to observe the full state, i.e., environment is partially observed, the problem is modelled as a partially observable MDP (POMDP), which is a generalize of the traditional one. It still possesses Markov property, but can only get hold of the partial observation $o$ instead of the state $s$. For instance, a robot only have limited information of the environment from, say, sensors. POMDP can be described by $(S, A, T, R, \Omega, O, \gamma)$, with the elements representing the state space (implicit variable), action space, state transition probability $T(s'\vert s, a)$, reward function, observation probability $\Omega(o\vert s, a)$, observation space and discount factor respectively.

## Major Components of Agent

Namely, *policy function*, *value function* and *model*.

### Policy Function

Policy is essentially the behavior model of an agent, in the form of a function transforming the inputted state to an action. There are 2 types of policy:

- **Stochastic policy** $\pi(a\vert s) = P[A_t=a\vert S_t=s]$, action determined by sampling from the probability distribution
- **Deterministic policy** $a*=\arg \max \pi(a\vert s)$

The former one is commonly used in RL because

- Randomness $\to$ more exploration
- A variety of actions can be taken for the same policy under the same state. This is particularly useful in the case of a multi-agent environment, where reacting consistently will make an agent's policy predictable to the others.

### Value Function

A value function predicts the future reward to decide how "favorable" a state is. It introduces a discounter factor, which explains how a reward becomes less valuable as time elapses. 


$$
v_\pi(s) = \mathbb{E}_\pi[G_t\vert S_t = s] = \mathbb{E}_\pi\left[\sum^{\infty}_{k=0}\gamma^kR_{t+k+1} \mid S_t = s\right])
$$

Another value function (aka the $Q$ function) computes the expected future reward based on both the state and action. 


$$
q_\pi(s, a) = \mathbb{E}_\pi[G_t \vert S_t=s, A_t=a] = \mathbb{E}_\pi\left[\sum^{\infty}_{k=0}\gamma^kR_{t+k+1} \mid S_t = s, A_t=a\right])
$$

An agent needs to learn the $Q$  function through training, so that an action that would result in the maximum reward can be deduced at each state.

### Model

Model determines what the next state will be based on the current state and action taken. It consists of a state transition probability

$$
\mathcal{P}^a_{ss'} = \mathbb{P}[S_{t+1}=s'\mid S_t=s, A_t=a]
$$
and a reward function.

$$
\mathcal{R}^a_s = \mathbb{E}[R_{t+1} \mid S_t=s, A_t=a]
$$

## Types of Algorithms

Based on **what agents learn**, RL algorithms can be categorized into policy-based and value-based methods. The former (e.g., policy gradient (PG) methods) directly gives the best action at a given state, whereas the later (e.g., Q-learning, Sarsa) assigns a value to each state and derive an action from the value differences. The value-based RL algorithms cannot (or are better not to) be applied to a continuous action space due to its complexity.

An Actor-Critic algorithm takes advantage of both the policy-based and value-based methods. Agent would act based on the policy, while the value function will assign a value to the action taken. This can accelerate the learning process on the basis of the original PG algorithms, hence achieving better learning outcomes.

Based on **agents' knowledge of the environment**, the algorithms can also be categorized into model-free (e.g., DQN, AlphaGo) and model-based methods. The former does not directly estimate or get to know how the state changes: it makes decisions based on the value and policy functions, hence requiring no model for state transition. For the later, when $P(s_{t+1}\vert s_t, a_t)$ and $R(s_t, a_t)$ are known, the agent can deduce the reward and next state resulted from an action under a certain state. Therefore, instead of interacting with the actual environment, the agent can simply learn in a "virtual world".

However, it is normally not feasible to estimate the state transition function and the reward function, sometimes also the state of environment. Hence, model-free methods are used in such cases  even though they are more data-driven than their model-based counterparts. Note that a model-based method without doing the modelling part is essentially model-free.

## Comparison with Supervised Learning

Supervised learning methods are based on the hypotheses

- Input data are independent, i.e., they are i.i.d. (independent & identically distributed). 
- A label is provided with each datum.

Both are necessary to train a classifier (e.g., a neural network) via backpropagation of error between the true value (i.e., the label) and the predicted one.

Apparently, none of the hypotheses can apply to the problems RL address because its inputs are time-series data, which would result in a **delayed reward.** This implies, the series of actions taken will only be judged of its contribution in obtaining the reward after a game is ended. So reinforcement learning in short is to sample actions until game is over, then penalize each action. In addition, no "correct values" can be fed to the agents. They must explore via trial-and-error to discover the best action to take under a given state. Hence, one needs to strike a balance between *exploration* and *exploitation*, where the former takes some "risky" moves and the later maintains the old decision-making policy to guarantee an attained reward. 

**Remark** Same as deep learning to traditional machine learning, deep RL adopts neural networks to achieve end-to-end training, without the need of feature engineering. It can optimize feature extraction and evaluation (or decision-making policy) simultaneously, resulting in a stronger policy network.

