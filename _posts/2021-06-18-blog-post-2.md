---
title: 'Study Notes: GNN Part (II) ðŸŒ² Message Passing Networks'
date: 2021-06-18
permalink: /posts/2021/06/gnn-2/
categories:
  - Computer Science
tags:
  - GNN
---

Introduce **neighborhood aggregation** / **message passing** scheme that is commonly applied to generalize the convolution operator to irregular domains. Illustrate the `MessagePassing` base class in `PyG`, and provide implementation of the *GCN layer* and the *edge convolution*.

Credit ðŸ”¥ PyTorch Geometric (`PyG`) provides a very detailed [documentation](https://pytorch-geometric.readthedocs.io/en/latest/notes/create_gnn.html#creating-message-passing-networks) for creating message passing networks via the base class `MessagePassing`.

![](/assets/img/banner-2.jpg)

## Message Passing Scheme

*Pipeline*:$\quad$Transform features of adjacent nodes $\to$ Aggregate $\to$ Update the central node

To put things mathematically, denote node features of node $i$ in layer $(k-1)$ by $x_i^{(k-1)}\in \mathbb{R}^F$ and (optional) edge features from node $j$ to node $i$ by $\mathbf{e}_{j, i}\in\mathbb{R}^D$. Message passing GNNs can be described as

$$
\mathbf{x}_i^{(k)} = \gamma^{(k)} \left( \mathbf{x}_i^{(k-1)}, \square_{j \in \mathcal{N}(i)} \, \phi^{(k)}\left(\mathbf{x}_i^{(k-1)}, \mathbf{x}_j^{(k-1)},\mathbf{e}_{j,i}\right) \right),
$$

where $\square$ denotes a differentiable, permutation invariant function, e.g., sum, mean or max, and $\gamma$ (for update) and $\phi$ (for message construction) denote differentiable functions such as MLPs.

## The `MessagePassing` Base Class

The `MessagePassing` class in `PyG` helps in creating message passing GNNs by automatically taking care of message propagation. The key methods (which should be re-implemented by users) are `message()` and `update()`. Aggregation scheme to use can be defined when initializing an instance of the class, e.g., `MessagePassing(aggr="add", flow="source_to_target", node_dim=-2)`. The method `propagate()` makes the initial call to start propagating messages.

**Remark** $\quad$`propagate()` checks if the argument `edge_index` is of the `SparseTensor` type and if the `message_and_aggregate()` method has been implemented in the derived class. If yes, then the more time efficient `message_and_aggregate()` is called instead of `message()`, `aggregate()` and `update()` sequentially. `

## Implementing the GCN Layer

GCN layer:

$$
\mathbf{x}_i^{(k)} = \sum_{j \in \mathcal{N}(i) \cup \{ i \}} \frac{1}{\sqrt{\deg(i)} \cdot \sqrt{\deg(j)}} \cdot \left( \mathbf{\Theta} \cdot \mathbf{x}_j^{(k-1)} \right),
$$

where neighboring node features are first transformed by a weight matrix $\mathbf{\Theta}$, normalized by their degree, and finally summed up. In details,

1. Add self-loops to the adjacency matrix.
2. Linearly transform node feature matrix.
3. Compute normalization coefficients.
4. Normalize node features in $\phi$.
5. Sum up neighboring node features. (`"add"` aggregation)

Steps 1-3 are typically computed before message passing takes place. Steps 4-5 can be easily processed with the `MessagePassing` base class. 

```python
import torch
from torch_geometric.nn import MessagePassing
from torch_geometric.utils import add_self_loops, degree

class GCNConv(MessagePassing):
    
    def __init__(self, in_channels, out_channels):
        super(GCNConv, self).__init__(aggr='add')  # "Add" aggregation (Step 5).
        self.linear = torch.nn.Linear(in_channels, out_channels)

    def forward(self, x, edge_index):
        # x has shape [N, in_channels]
        # edge_index has shape [2, E]

        # Step 1: Add self-loops to the adjacency matrix.
        edge_index, _ = add_self_loops(edge_index, num_nodes=x.size(0))

        # Step 2: Linearly transform node feature matrix.
        x = self.linear(x)

        # Step 3: Compute normalization.
        row, col = edge_index
        deg = degree(col, x.size(0), dtype=x.dtype)
        deg_inv_sqrt = deg.pow(-0.5)
        deg_inv_sqrt[deg_inv_sqrt == float('inf')] = 0
        norm = deg_inv_sqrt[row] * deg_inv_sqrt[col]

        # Step 4-5: Start propagating messages.
        return self.propagate(edge_index, x=x, norm=norm)

    def message(self, x_j, norm):
        # x_j has shape [E, out_channels]

        # Step 4: Normalize node features.
        return norm.view(-1, 1) * x_j
```



## Implementing the Edge Convolution

The edge convolutional layer:

$$
\mathbf{x}_i^{(k)} = \max_{j\in\mathcal{N}(i)}h_{\mathbf{\Theta}}(\mathbf{x}_i^{(k-1)}, \mathbf{x}_j^{(k-1)}-\mathbf{x}_i^{(k-1)}),
$$

where $h_{\mathbf{\Theta}}$ denotes an MLP. This time we use the `"max"` aggregation.

```python
import torch
import torch.nn as nn
from torch_geometric.nn import MessagePassing

class EdgeConv(MessagePassing):
    
    def __init__(self, in_channels, out_channels):
        super(EdgeConv, self).__init__(aggr="max")
        self.mlp = nn.Sequential(
            nn.Linear(2 * in_channels, out_channels),
            nn.ReLU(),
            nn.Linear(out_channels, out_channels)
        )

    def forward(self, x, edge_index):
        return self.propagate(edge_index, x=x)

    def message(self, x_i, x_j):
        cat = torch.cat([x_i, x_j - x_i], dim=1)
        return self.mlp(cat)
```

The edge convolution is actually a dynamic convolution, which recomputes the graph for each layer using nearest neighbors in the feature space. `torch_geometric.nn.pool.knn_graph()` can compute a nearest neighbor graph, which will be further used to call the `forward()` method of `EdgeConv`.

```python
from torch_geometric.nn import knn_graph

class DynamicEdgeConv(EdgeConv):
    def __init__(self, in_channels, out_channels, k=6):
        super(DynamicEdgeConv, self).__init__(in_channels, out_channels)
        self.k = k

    def forward(self, x, batch=None):
        edge_index = knn_graph(x, self.k, batch, loop=False, flow=self.flow)
        return super(DynamicEdgeConv, self).forward(x, edge_index)
```

