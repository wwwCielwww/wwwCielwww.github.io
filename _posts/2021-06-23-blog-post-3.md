---
title: 'Study Notes: GNN Part (III) ðŸŒ² Node Representation Learning'
date: 2021-06-23
permalink: /posts/2021/06/gnn-3/
categories:
  - Computer Science
tags:
  - GNN

---

Experiments with MLP, GCN and GAT on the citation network dataset "Cora", with the graph's nodes representing the documents and the edges representing the citation links. 

### Dataset Info

```python
from torch_geometric.datasets import Planetoid
from torch_geometric.transforms import NormalizeFeatures

dataset = Planetoid("dataset", "Cora", transform=NormalizeFeatures())
data = dataset[0]
```

| No. Graphs             | No. Feature Dimensions       | No. Classes                 | No. Nodes              | No. Edges         |
| ---------------------- | ---------------------------- | --------------------------- | ---------------------- | ----------------- |
| 1                      | 1433                         | 7                           | 2708                   | 10556             |
| **No. Training Nodes** | **Training Node Label Rate** | **Contains Isolated Nodes** | **Contain Self-Loops** | **Is Undirected** |
| 140                    | 0.05                         | False                       | False                  | True              |

### General Settings

```python
criterion = torch.nn.CrossEntropyLoss()
optimizer = torch.optim.Adam(model.parameters(), lr=0.01,
weight_decay=5e-4) 
```

### Boilerplate Code for Training & Testing

```python
def train():
    model.train()
    optimizer.zero_grad() 
    out = model(data.x) 
    loss = criterion(out[data.train_mask], data.y[data.train_mask]) 
    loss.backward() 
    optimizer.step() 
    return loss

def test():
    model.eval()
    out = model(data.x)
    pred = out.argmax(dim=1) 
    test_correct = pred[data.test_mask] == data.y[data.test_mask]
    test_acc = int(test_correct.sum()) / int(data.test_mask.sum())
    return test_acc
```

### Preparation - Visualize Node Feature Distribution Across Different Dimensions

```python
import matplotlib.pyplot as plt
from sklearn.manifold import TSNE

def visualize(out, color):
    z = TSNE(n_components=2).fit_transform(out.detach().cpu().numpy())
	plt.figure(figsize=(10,10))
	plt.xticks([])
	plt.yticks([])
	plt.scatter(z[:, 0], z[:, 1], s=70, c=color, cmap="Set2")
	plt.show()
```

`TSNE` $\to$ **t-distributed Stochastic Neighbor Embedding**

is a tool to visualize high-dimensional data. It converts similarities between data points to joint probabilities and tries to minimize the Kullback-Leibler divergence between the joint probabilities of the low-dimensional embedding and the high-dimensional data. t-SNE has a cost function that is not convex, i.e. with different initializations one can get different results.

## Our Old Friend, MLP

In the case of `Cora`, as a citation network, we may consider the task of node classification as a classic one, where we can feed the plain texts of documents into an MLP and obtain the categories of the documents. Note that information on edges has been neglected and hence, all nodes share the same weights of the network.

```python
import torch
from torch.nn import Linear
import torch.nn.functional as F

class MLP(torch.nn.Module):
	def __init__(self, num_features, num_classes, hidden_channels):
		super(MLP, self).__init__()
		self.lin1 = Linear(num_features, hidden_channels)
		self.lin2 = Linear(hidden_channels, num_classes)
	def forward(self, x):
		x = self.lin1(x)
		x = x.relu()
		x = F.dropout(x, p=0.5, training=self.training)
		x = self.lin2(x)
		return x
```

## Graph Convolutional Network (GCN)



## Graph Attention Network (GAT)

