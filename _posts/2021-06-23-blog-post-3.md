---
title: 'Study Notes: GNN Part (III) ðŸŒ² Node Representation Learning'
date: 2021-06-23
permalink: /posts/2021/06/gnn-3/
categories:
  - Computer Science
tags:
  - GNN
---

Experiment with MLP, GCN and GAT on the citation network dataset "Cora", where the graph's nodes represent the documents and the edges represent the citation links. [**Updating**...]

View the code at [Google Colab](https://colab.research.google.com/drive/1O3Ym-xfVvd_IyJUQkTbMLsvlpuV_FjwA?usp=sharing) / Download the .ipynb file [here](https://wwwCielwww.github.io/file/MLP_GCN_GAT_On_Cora.ipynb)

### Dataset Info

```python
from torch_geometric.datasets import Planetoid
from torch_geometric.transforms import NormalizeFeatures

dataset = Planetoid("dataset", "Cora", transform=NormalizeFeatures())
data = dataset[0]
```

| No. Graphs             | No. Feature Dimensions       | No. Classes                 | No. Nodes              | No. Edges         |
| ---------------------- | ---------------------------- | --------------------------- | ---------------------- | ----------------- |
| 1                      | 1433                         | 7                           | 2708                   | 10556             |
| **No. Training Nodes** | **Training Node Label Rate** | **Contains Isolated Nodes** | **Contain Self-Loops** | **Is Undirected** |
| 140                    | 0.05                         | False                       | False                  | True              |

### General Settings

```python
criterion = torch.nn.CrossEntropyLoss()
optimizer = torch.optim.Adam(model.parameters(), lr=0.01,
weight_decay=5e-4) 
hidden_channels = 16
```

### Boilerplate Code for Training & Testing

```python
def train():
    model.train()
    optimizer.zero_grad() 
    out = model(data.x) 
    loss = criterion(out[data.train_mask], data.y[data.train_mask]) 
    loss.backward() 
    optimizer.step() 
    return loss

def test():
    model.eval()
    out = model(data.x)
    pred = out.argmax(dim=1) 
    test_correct = pred[data.test_mask] == data.y[data.test_mask]
    test_acc = int(test_correct.sum()) / int(data.test_mask.sum())
    return test_acc
```

### Preparation - Visualize Node Feature Distribution Across Different Dimensions

```python
import matplotlib.pyplot as plt
from sklearn.manifold import TSNE

def visualize(out, color):
    z = TSNE(n_components=2).fit_transform(out.detach().cpu().numpy())
	plt.figure(figsize=(10,10))
	plt.xticks([])
	plt.yticks([])
	plt.scatter(z[:, 0], z[:, 1], s=70, c=color, cmap="Set2")
	plt.show()
```

`TSNE` $\to$ **t-distributed Stochastic Neighbor Embedding**

is a tool to visualize high-dimensional data. It converts similarities between data points to joint probabilities and tries to minimize the Kullback-Leibler divergence between the joint probabilities of the low-dimensional embedding and the high-dimensional data. t-SNE has a cost function that is not convex, i.e. with different initializations one can get different results.

## Our Old Friend, MLP

In the case of `Cora`, as a citation network, we may consider the task of node classification as a classic one, where we can feed the plain texts of documents into an MLP and obtain the categories of the documents. Note that information on edges has been neglected and hence, all nodes share the same weights of the network.

```python
class MLP(torch.nn.Module):
	def __init__(self, num_features, num_classes, hidden_channels):
		super(MLP, self).__init__()
		self.lin1 = Linear(num_features, hidden_channels)
		self.lin2 = Linear(hidden_channels, num_classes)
	
    def forward(self, x):
		x = self.lin1(x)
		x = x.relu()
		x = F.dropout(x, p=0.5, training=self.training)
		x = self.lin2(x)
		return x
```

## Graph Convolutional Network (GCN)

*For more details, see the paper "[Semi-Supervised Classification with Graph Convolutional Networks](https://arxiv.org/abs/1609.02907)".*

For a signal $X\in \mathbb{R}^{N\times C}$ with $C$ input channels and $F$ filters, define the convolved signal matrix $Z\in\mathbb{R}^{N\times F}$:

$$
Z=\tilde{D}^{-\frac{1}{2}}\tilde{A}\tilde{D}^{-\frac{1}{2}}X\Theta.
$$

Here, $\tilde{A}=A+I_N$ is the adjacency matrix with added self-connections,  $\tilde{D}$ is the degree matrix of $\tilde{A}$ and $\Theta\in\mathbb{R}^{C\times F}$ is a matrix of filter parameters. For the feature vector of a single node $i$, 

$$
z_i = (\sum_{j\in\mathcal{N}(i)}\frac{e_{j, i}}{\sqrt{d_jd_i}}x_j)\Theta.
$$

```python
from torch_geometric.nn import GCNConv

class GCN(torch.nn.Module):
	def __init__(self, num_features, num_classes, hidden_channels):
        super(GCN, self).__init__()
        self.conv1 = GCNConv(dataset.num_features,
        hidden_channels)
        self.conv2 = GCNConv(hidden_channels, dataset.num_classes)
        
    def forward(self, x, edge_index):
        x = self.conv1(x, edge_index)
        x = x.relu()
        x = F.dropout(x, p=0.5, training=self.training)
        x = self.conv2(x, edge_index)
        return x
```

## Graph Attention Network (GAT)

*For more details, see the paper "[Graph Attention Networks](https://arxiv.org/abs/1609.02907)".*

In order to obtain sufficient expressive power to transform the input features into higher-level features, at least one learnable linear transformation is required. To that end, as an initial step, a shared
linear transformation, parametrized by a weight matrix, $W\in\mathbb{R}^{F\times C}$ is applied to every node. We then perform *self-attention* on the nodes â€” a shared attentional mechanism $a:\mathbb{R}^F\times \mathbb{R}^F\to\mathbb{R}$ computes *attention coefficients*

$$
e_{ij}=a(Wx_i, Wx_j)
$$

that indicate the *importance* of node $j$'s features to node $i$. In its most general formulation, the model allows every node to attend on every other node, dropping all structural information. We inject the graph structure into the mechanism by performing *masked attention* â€” we only compute $e_{ij}$ for nodes $j\in\mathcal{N}_i$. To make coefficients easily comparable across different nodes, we normalize them across all choices of $j$ using the softmax function:

$$
\alpha_{ij} = \mathrm{softmax}_j(e_{ij}) = \frac{\mathrm{exp}(e_{ij})}{\sum_{k\in\mathcal{N}_i}\mathrm{exp}(e_{ik})}.
$$

In the original experiments, the attention mechanism $a$ is a single-layer feedforward neural network, parametrized by a weight vector $w\in\mathbb{R}^{2F}$, and applying the LeakyRelu nonlinearity. Fully expanded out, the coefficients computed by the attention mechanism may be expressed as:

$$
\alpha_{i,j} = \frac{ \exp\left(\mathrm{LeakyReLU}\left(a^T[W x_i \, \Vert \, W x_j]\right)\right)}
{\sum_{k \in \mathcal{N}(i)}\exp\left(\mathrm{LeakyReLU}\left(a^T[W x_i \, \Vert \, Wx_k]\right)\right)}.
$$

where $.^T$ represents transposition and $\Vert$ is the concatenation operation.

Once obtained, the normalized attention coefficients are used to compute a linear combination of the features corresponding to them, to serve as the final output features for every node (after potentially applying a nonlinearity,  $\sigma$):

$$
z_i = \sigma(\sum_{j\in\mathcal{N}(i)}\alpha_{i,j}W x_j).
$$

```python
from torch_geometric.nn import GATConv

class GAT(torch.nn.Module):
    def __init__(self, hidden_channels):
        super(GAT, self).__init__()
        self.conv1 = GATConv(dataset.num_features, hidden_channels)
        self.conv2 = GATConv(hidden_channels, dataset.num_classes)

    def forward(self, x, edge_index):
        x = self.conv1(x, edge_index)
        x = x.relu()
        x = F.dropout(x, p=0.5, training=self.training)
        x = self.conv2(x, edge_index)
        return x
```



