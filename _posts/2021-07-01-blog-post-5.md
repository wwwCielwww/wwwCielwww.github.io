---
title: 'Study Notes: GNN Part (V) üå≤ Node Representation Learning on Large Graphs'
date: 2021-07-01
permalink: /posts/2021/07/gnn-5/
categories:
  - Computer Science
tags:
  - GNN

---

Explain the difficulty of training deep and large graph convolutional networks; Introduce Cluster-GCN ‚Äî an approach with a key idea of merging partitioned subgraphs and their between-cluster links from a large scale data object to form a minibatch, which is easier to train. 

- View the code for Cluster-GNN at [Google Colab](https://colab.research.google.com/drive/1M1IHqbC6WmC1CxAzCRM_xEyeNLClk_xV?usp=sharing) / Download the .ipynb file [here](https://wwwcielwww.github.io/files/Cluster_GCN.ipynb)

## Problems with Training on Very Large Graphs

$\to$ As the depth of GNN increases, the computational cost increases exponentially.

$\to$ Saving the entire graph and the node representations of each layer to CPU/GPU will consume considerable CPU/GPU space.

$\to$ Not saving all the information mentioned above might result in decrease in accuracy or have no significant impact on cutting the space consumption.

The paper [*Cluster-GCN: An Efficient Algorithm for Training Deep and Large Graph Convolutional Networks*](https://arxiv.org/abs/1905.07953)  has introduced a novel approach to train such graphs. 

## The Gist of Cluster-GCN

- Apply cluster algorithms to partition a graph's nodes into clusters;

- Each time choose the nodes of several clusters and the corresponding edges to form a subgraph for training;

**How it works?**

- There are far more intra-cluster edges than inter-cluster ones $\implies$ we can neglect the inter-cluster edges and focus within the clusters. This will  result in a higher "utility rate" of embeddings, i.e., embeddings in layer $l$  are more likely to to be used again in layer $l+1$.
- Randomly selecting multiple clusters to form a batch will give us more inter-cluster links and balance the classes within the batch.
- With smaller graphs consumes less CPU/GPU space. Therefore, deeper neural networks can be trained for higher accuracy.

## Let's Take a Closer Look 

Let's first review the traditional GCN:

$$
Z^{(l+1)} = A'X^{(l)}W^{(l)}, \ X^{(l+1)} = \sigma(Z^{(l+1)})
$$

defines the forward propagation from layer $l$ to $l+1$, where $W^{(l)}\in\mathbb{R}^{F_lF_{l+1}}$ is the weight matrix (i.e., of the trainable parameters) and $\sigma$ is the activation function. And

$$
\mathcal{L}=\frac{1}{\mid\mathcal{Y}_L\mid}\sum_{i\in\mathcal{Y}_L}\mathrm{loss}(y_i, z_i^L)
$$

defines the loss function to be minimized, where $\mathcal{Y}_L$ contains all the labels for the labeled nodes and $z^L_i$ is the $i^\mathrm{th}$ row of the final layer's output, resembling the model's prediction of node $i$'s class. 

- To store all the embeddings, a space of $O(NFL)$ is needed.
- The model is only updated once for one epoch $\implies$ more epochs is needed for convergence.

### Vanilla Mini-Batch SGD

It's straightforward to deduce that training with mini-batch can have less space consumption and converge with fewer epochs. However, more time is taken with each epoch. (Why?)

---

***Analysis***  We consider the computation of the gradient associated with
one node $i$: $\nabla\mathrm{loss}(y_i, z_i^{(l)})$. Clearly, this requires the embedding of node $i$, which depends on its neighbors‚Äô embeddings in the previous layer (i.e., field extension). To fetch each node $i$'s neighbor nodes‚Äô embeddings, we need to further aggregate each neighbor node‚Äôs neighbor nodes‚Äô embeddings as well. Suppose a GCN has $L + 1$ layers and each node has an average degree of $d$, to get the gradient for node $i$, we need to aggregate features from $O(d^L)$ nodes in the graph for one node. That is, we need to fetch information for a node‚Äôs hop-$k \ (k = 1, \dots , L)$ neighbors in the graph to perform one update. Computing each embedding requires $O(F^2)$ time due to the multiplication with $W^{(l)}$, so in average computing the gradient associated with one node requires $O(d^LF^2)$ time.

**Embedding utilization can reflect computational efficiency.** If a batch has more than one node, the time complexity is less straightforward since different nodes can have overlapped hop-$k$ neighbors, and the number of embedding computation can be less than the worst case $O(bd^L)$. To reflect the computational efficiency of mini-batch SGD, we define the concept of ‚Äú**embedding utilization**‚Äù to characterize the computational efficiency. If the node $i$‚Äôs embedding at $l$-th layer $z_i^{(l)}$ is computed and is reused $u$ times for the embedding computations at layer $l + 1$, then we say the embedding utilization of $z_i^{(l)}$ is $u$. For mini-batch SGD with random sampling, $u$ is very small since the graph is usually large and sparse. Assume $u$ is a small constant (almost no overlaps between hop-$k$ neighbors), then mini-batch
SGD needs to compute  $O(bd^L)$ embeddings per batch, which leads
to  $O(bd^LF^2)$  time per update and $O(Nd^LF^2)$ time per epoch.

In contrary, full-batch gradient descent has the maximal embedding utilization ‚Äî each embedding will be reused $d$ (average degree) times in the upper layer. As a consequence, the original full gradient descent only needs to compute $O(NL)$ embeddings per epoch, which means on average only $O(L)$ embedding computation is needed to acquire the gradient of one node.

### Improvement Made by Cluster-GCN

Cluster-GCN wants to maximize the embedding utilization. It's not hard to see that the embedding utilization for a batch of nodes is the number of edges between them. Therefore, our goal is equivalent to maximizing the number of edges within a batch / minimizing the number of edges existing between batches, which coincides with the problem that clustering dedicates to solve. 

With $c$ partitioned subgraphs, the adjacency matrix can be written as the block matrix

$$
A=\bar{A}+\Delta=
\left[\begin{array}{ccc}
A_{11} & \cdots & 0 \\
\vdots & \ddots & \vdots \\
0 & \cdots & A_{c c}
\end{array}\right] + \left[\begin{array}{ccc}
0 & \cdots & A_{1 c} \\
\vdots & \ddots & \vdots \\
A_{c 1} & \cdots & 0
\end{array}\right] =
\left[\begin{array}{ccc}
A_{11} & \cdots & A_{1 c} \\
\vdots & \ddots & \vdots \\
A_{c 1} & \cdots & A_{c c}
\end{array}\right],
$$

where $\tilde{A}$ is a block diagonal matrix of subgraphs' adjacency matrices and $\Delta$ includes the edges existing between subgraphs. With a negligible $\Delta$, we can approximate $A$ by $\tilde{A}$. This will allow the loss function to be written as a sum of the batches' individual ones:

$$
\begin{aligned}
Z^{(L)} &=\bar{A}^{\prime} \sigma\left(\bar{A}^{\prime} \sigma\left(\cdots \sigma\left(\bar{A}^{\prime} X W^{(0)}\right) W^{(1)}\right) \cdots\right) W^{(L-1)} \\
&=\left[\begin{array}{c}
\bar{A}_{11}^{\prime} \sigma\left(\bar{A}_{11}^{\prime} \sigma\left(\cdots \sigma\left(\bar{A}_{11}^{\prime} X_{1} W^{(0)}\right) W^{(1)}\right) \cdots\right) W^{(L-1)} \\
\vdots \\
\bar{A}_{c c}^{\prime} \sigma\left(\bar{A}_{c c}^{\prime} \sigma\left(\cdots \sigma\left(\bar{A}_{c c}^{\prime} X_{c} W^{(0)}\right) W^{(1)}\right) \cdots\right) W^{(L-1)}
\end{array}\right]
\end{aligned}
$$

Hence, the loss function can be decomposed into

$$
\mathcal{L}_{\bar{A}^{\prime}}=\sum_{t} \frac{\left|\mathcal{V}_{t}\right|}{N} \mathcal{L}_{\bar{A}_{t t}^{\prime}} \text { and } \mathcal{L}_{\bar{A}_{t t}^{\prime}}=\frac{1}{\left|\mathcal{V}_{t}\right|} \sum_{i \in \mathcal{V}_{t}} \operatorname{loss}\left(y_{i}, z_{i}^{(L)}\right).
$$

Therefore, in training, the model's parameters can be updated based on the gradient of individual loss function for each cluster $V_t$. For each update, only $A_{tt}, X_t, Y_t$ and $\{W\}^L_{l=1}$ will be used.

<center><sub>Time & Space Complexity of Various GNNs</sub></center>

![](/assets/img/time-space-complexity.png)

### Problems & Solution

üôÖ The nodes of the same cluster are more likely to be of the same class $\implies$ class distribution may be very different from a cluster to another $\implies$ impact on training's convergence

üôÖ Some between-cluster links are neglected when approximating $A$ by $\tilde{A}$.

‚ñ∂Ô∏è **Mini-batch of randomly selected clusters!**

Specifically, in each epoch, we will randomly select several clusters and the edges between them to form a batch. This trick was tested on the Reddit dataset and turned out to converge must faster than its single-cluster counterpart. 

üôÖ Deep GCNs are prone to overfitting and the loss of "shallow" information. 

‚ñ∂Ô∏è **Directly send a layer's information to the next!**

Well, how should we do this? Some adopted a technique similar to residual connections, whereas Cluster-GCN proposed an approach in which the diagonal of the adjacency matrix $A$ is enlarged in value at each layer. This will give a higher weight to the representations of the previous layer. For instance,

$$
X^{(l+1)}=\sigma\left(\left(A^{\prime}+I\right) X^{(l)} W^{(l)}\right).
$$

To be more sensible, the neighborhoods information and numerical ranges (values grow exponentially with layers) can affect the importance of a previous layer's node. Therefore, We first add an identity to the original $A$ and perform the normalization

$$
\tilde{A}=(D+I)^{-1}(A+I)
$$

, and then consider

$$
X^{(l+1)}=\sigma\left((\tilde{A}+\lambda \operatorname{diag}(\tilde{A})) X^{(l)} W^{(l)}\right).
$$