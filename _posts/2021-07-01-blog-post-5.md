---
title: 'Study Notes: GNN Part (V) ðŸŒ² Node Representation Learning on Large Graphs'
date: 2021-07-01
permalink: /posts/2021/07/gnn-5/
categories:
  - Computer Science
tags:
  - GNN

---

Explain the difficulty of training deep and large graph convolutional networks; Introduce Cluster-GCN - an approach with a key idea of merging partitioned subgraphs and their between-cluster links from a large scale data object to form a minibatch, which is easier to train. 

## Problems with Training on Very Large Graphs

$\to$ As the depth of GNN increases, the computational cost increases exponentially.

$\to$ Saving the entire graph and the node representations of each layer to CPU/GPU will consume considerable CPU/GPU space.

$\to$ Not saving all the information mentioned above might result in decrease in accuracy or have no significant impact on cutting the space consumption.

The paper [*Cluster-GCN: An Efficient Algorithm for Training Deep and Large Graph Convolutional Networks*](https://arxiv.org/abs/1905.07953)  has introduced a novel approach to train such graphs. 

## The Gist of Cluster-GCN

- Apply cluster algorithms to partition a graph's nodes into clusters;

- Each time choose the nodes of several clusters and the corresponding edges to form a subgraph for training;

**How it works?**

- There are far more intra-cluster edges than inter-cluster ones $\implies$ we can neglect the inter-cluster edges and focus within the clusters. This will  result in a higher "utility rate" of embeddings, i.e., embeddings in layer $l$  are more likely to to be used again in layer $l+1$.
- Randomly selecting multiple clusters to form a batch will give us more inter-cluster links and balance the classes within the batch.
- With smaller graphs consumes less CPU/GPU space. Therefore, deeper neural networks can be trained for higher accuracy.

## A Closer Look ...

Let's first review the traditional GCN:
$$
Z^{(l+1)} = A'X^{(l)}W^{(l)}, \ X^{(l+1)} = \sigma(Z^{(l+1)})
$$
defines the forward propagation from layer $l$ to $l+1$, where $W^{(l)}\in\mathbb{R}^{F_lF_{l+1}}$ is the weight matrix (i.e., of the trainable parameters) and $\sigma$ is the activation function. And
$$
\mathcal{L}=\frac{1}{\mathcal{Y}_L}\sum_{y_i\in\mathcal{Y}_L}\mathrm{loss}(y_i, z_i^L)
$$
defines the loss, where $\mathcal{Y}_L$ is the set of node classes and $z^L_i$ is the $i^\mathrm{th}$ row of the last layer's output, resembling the model's prediction of node $i$'s class. 

- To store all the embeddings, a space of $O(NFL)$ is needed.
- The model is only updated once for one epoch $\implies$ more epochs is needed for convergence.

### Mini-Batch Training Issue

It's straightforward to deduce that training with mini-batch can have less space consumption and converge with fewer epochs. However, more time is taken with each epoch. (Why?)

<details>
    <summary>hi</summary>
    qwq
</details>

