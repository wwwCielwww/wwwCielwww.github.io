---
title: 'Study Notes: GNN Part (VI) ðŸŒ² Graph Representation Learning'
date: 2021-07-05
permalink: /posts/2021/07/gnn-6/
categories:
  - Computer Science
tags:
  - GNN
---

Introduce graph representation learning via GNN, with inputs of node attributes, edge indices and edge attributes (if any); Explain Graph Isomorphism Network (GIN), one of the classic algorithm of doing so.

## Graph Isomorphism Network - An Introduction

**How it's done?**

- Compute node embeddings based on information on nodes and edges;
- Conduct *Graph Pooling* / *Graph Readout* (basically the same, e.g., sum, mean, max, using [attention](https://arxiv.org/abs/1511.05493) mechanism or [set2set](https://arxiv.org/abs/1511.06391)) on the node embeddings to obtain the graph embedding;
- (for node classification) Feed into a classifier (e.g., `nn.Linear(emb_dim, num_classes)`).

## GIN's Node Embedding Module

The module can be used in the first step to compute node embeddings. 

We first encode the node attributes to obtain embeddings of $0^{\mathrm{th}}$ layer via `AtomEncoder`, and then propagate through the network of `GINConv` layers. With a deeper network increases the receptive field: a node's embedding can contain information regarding neighboring nodes with a distance less than `num_layers`. 

### GINConv Layer

Mathematical definition:

$$
\mathbf{x}^{\prime}_i = h_{\mathbf{\Theta}} \left( (1 + \epsilon) \cdot
\mathbf{x}_i + \sum_{j \in \mathcal{N}(i)} \mathbf{x}_j \right)
$$

To add support for edge attributes, we define a GINConv module as below. Note that calling `propagate` will in turn, call `message `, `aggregate` and `update`. `x_i` , `x_j` are tensors of the target and source nodes of message passing. 

```python
class GINConv(MessagePassing):
    def __init__(self, emb_dim):
        super(GINConv, self).__init__(aggr = "add")

        self.mlp = nn.Sequential(nn.Linear(emb_dim, emb_dim), nn.BatchNorm1d(emb_dim), nn.ReLU(), nn.Linear(emb_dim, emb_dim))
        self.eps = nn.Parameter(torch.Tensor([0]))
        self.bond_encoder = BondEncoder(emb_dim = emb_dim)

    def forward(self, x, edge_index, edge_attr):
        edge_embedding = self.bond_encoder(edge_attr) 
        out = self.mlp((1 + self.eps) *x + self.propagate(edge_index, x=x, edge_attr=edge_embedding))
        return out

    def message(self, x_j, edge_attr):
        return F.relu(x_j + edge_attr)
        
    def update(self, aggr_out):
        return aggr_out
```

---

After generating node embeddings, the result will undergo a pooling / readout layer and concatenated in the dimension of layers. Mathematically,

$$
h_{G} = \text{CONCAT}(\text{READOUT}\left(\{h_{v}^{(k)}|v\in G\}\right)|k=0,1,\cdots, K)
$$

`concat` is used instead of simply adding the embeddings of different layers because they belong to different eigenspaces. Note that with (weighted) sum across the node dimension applied in the pooling operation, information on node distribution may be lost. In the paper [How Powerful are Graph Neural Networks?](https://arxiv.org/abs/1810.00826), such graph embedding obtained is shown to be equivalent to the one obtained in WL test (see below).

## Weisfeiler-Lehman (WL) Test & Subtree Kernel

WL test was designed to inspect structural similarity of two graphs, specifically, whether they are isomorphic or not. On 1-dimension, it iteratively aggregate labels of nodes with those of their neighbors, and then hash them into new labels. $h:=\text{number of iterations, } u:=\text{the node } u$

$$
L^{h}_{u} \leftarrow \operatorname{hash}\left(L^{h-1}_{u} + \sum_{v \in \mathcal{N}(U)} L^{h-1}_{v}\right)
$$

During each iteration, if the number of nodes with certain label differ between the two graphs, then we can conclude that they are not isomorphic. Otherwise, as a common practice, the graphs are regarded as being isomorphic after a certain (large) number of iterations. In the later case, the test would fail with e.g., complete graphs, cycle graphs and stars, which are of high symmetry. Note that the computation of node embedding in GIN follows from the update method of node label here.

On top of the WL Test, WL Subtree Kernel measures the similarity between two graphs. By using the labels generated in the iterations, we can count the number of  occurrences for each label and store them in a vector as the graph's representation. The higher the inner product of two graph's representation, the more similar they are.

---

*Assignment* (WL Subtree)

  6      3          5

  4    2  4      1  2  4

3  5  135 356   25 135 356







 

