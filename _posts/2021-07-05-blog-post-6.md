---
title: 'Study Notes: GNN Part (VI) ðŸŒ² Graph Representation Learning'
date: 2021-07-05
permalink: /posts/2021/07/gnn-6/
categories:
  - Computer Science
tags:
  - GNN
---

Introduce graph representation learning via GNN, with inputs of node attributes, edge indices and edge attributes (if any); Explain Graph Isomorphism Network (GIN), one of the classic algorithm of doing so.

## Graph Isomorphism Network - An Introduction

**How it's done?**

- Compute node embeddings based on information on nodes and edges;
- Conduct *Graph Pooling* / *Graph Readout* (basically the same, e.g., sum, mean, max, using [attention](https://arxiv.org/abs/1511.05493) mechanism or [set2set](https://arxiv.org/abs/1511.06391)) on the node embeddings to obtain the graph embedding;
- (for node classification) Feed into a classifier (e.g., `nn.Linear(emb_dim, num_classes)`).

## GIN's Node Embedding Module

The module can be used in the first step to compute node embeddings. 

We first encode the node attributes to obtain embeddings of $0^{\mathrm{th}}$ layer via `AtomEncoder`, and then propagate through the network of `GINConv` layers. With a deeper network increases the receptive field: a node's embedding can contain information regarding neighboring nodes with a distance less than `num_layers`. 

### GINConv Layer

Mathematical definition:

$$
\mathbf{x}^{\prime}_i = h_{\mathbf{\Theta}} \left( (1 + \epsilon) \cdot
\mathbf{x}_i + \sum_{j \in \mathcal{N}(i)} \mathbf{x}_j \right)
$$

To add support for edge attributes, we define a GINConv module as below. Note that calling `propagate` will in turn, call `message `, `aggregate` and `update`. `x_i` , `x_j` are tensors of the target and source nodes of message passing. 

```python
class GINConv(MessagePassing):
    def __init__(self, emb_dim):
        super(GINConv, self).__init__(aggr = "add")

        self.mlp = nn.Sequential(nn.Linear(emb_dim, emb_dim), nn.BatchNorm1d(emb_dim), nn.ReLU(), nn.Linear(emb_dim, emb_dim))
        self.eps = nn.Parameter(torch.Tensor([0]))
        # Bond encoder embeds the edge attributes
        self.bond_encoder = BondEncoder(emb_dim = emb_dim)

    def forward(self, x, edge_index, edge_attr):
        edge_embedding = self.bond_encoder(edge_attr) 
        out = self.mlp((1 + self.eps) *x + self.propagate(edge_index, x=x, edge_attr=edge_embedding))
        return out

    def message(self, x_j, edge_attr):
        return F.relu(x_j + edge_attr)
        
    def update(self, aggr_out):
        return aggr_out
```

---

After generating node embeddings, the result will undergo a pooling / readout layer and concatenated in the dimension of layers. Mathematically,

$$
h_{G} = \text{CONCAT}(\text{READOUT}\left(\{h_{v}^{(k)}|v\in G\}\right)|k=0,1,\cdots, K)
$$

`concat` is used instead of simply adding the embeddings of different layers because they belong to different eigenspaces. Note that with (weighted) sum across the node dimension applied in the pooling operation, information on node distribution may be lost. In the paper [How Powerful are Graph Neural Networks?](https://arxiv.org/abs/1810.00826), such graph embedding obtained is shown to be equivalent to the one obtained in WL test (see below).

### Weisfeiler-Lehman (WL) Test & Subtree Kernel

WL test was designed to inspect structural similarity of two graphs, specifically, whether they are isomorphic or not. On 1-dimension, it iteratively aggregate labels of nodes with those of their neighbors, and then hash them into new labels. $h:=\text{number of iterations, } u:=\text{the node } u$

$$
L^{h}_{u} \leftarrow \operatorname{hash}\left(L^{h-1}_{u} + \sum_{v \in \mathcal{N}(U)} L^{h-1}_{v}\right)
$$

During each iteration, if the number of nodes with certain label differ between the two graphs, then we can conclude that they are not isomorphic. Otherwise, as a common practice, the graphs are regarded as being isomorphic after a certain (large) number of iterations. In the later case, the test would fail with e.g., complete graphs, cycle graphs and stars, which are of high symmetry. Note that the computation of node embedding in GIN follows from the update method of node label here.

On top of the WL Test, WL Subtree Kernel measures the similarity between two graphs. By using the labels generated in the iterations, we can count the number of  occurrences for each label and store them in a vector as the graph's representation. The higher the inner product of two graph's representation, the more similar they are.

---

*Assignment* (WL Subtree)

|      |  6   |      |      |      |  3   |      |      |      |      |  5   |      |      |
| :--: | :--: | :--: | ---- | :--: | :--: | :--: | ---- | :--: | ---- | :--: | ---- | :--: |
|      |  4   |      |      |  3   |      |  4   |      |  1   |      |  2   |      |  4   |
|  3   |      |  5   |      | 135  |      | 356  |      |  25  |      | 135  |      | 356  |

### Generate Node Embeddings with GINConv

```python
class GINNodeEmbedding(torch.nn.Module):
    def __init__(self, num_layers, emb_dim, drop_ratio=0.5, JK="last", residual=False):
        super(GINNodeEmbedding, self).__init__()
        self.num_layers = num_layers
        self.drop_ratio = drop_ratio
        self.JK = JK
        self.residual = residual

        if self.num_layers < 2:
            raise ValueError("Number of GNN layers must be greater than 1.")
        
        # Atom encoder embeds node attributes (i.e., data.x)
        self.atom_encoder = AtomEncoder(emb_dim)

        self.convs = torch.nn.ModuleList()
        self.batch_norms = torch.nn.ModuleList()

        for layer in range(num_layers):
            self.convs.append(GINConv(emb_dim))
            self.batch_norms.append(torch.nn.BatchNorm1d(emb_dim))

    def forward(self, batched_data):
        x, edge_index, edge_attr = batched_data.x, batched_data.edge_index, batched_data.edge_attr

        h_list = [self.atom_encoder(x)]  
        for layer in range(self.num_layers):
            h = self.convs[layer](h_list[layer], edge_index, edge_attr)
            h = self.batch_norms[layer](h)
            if layer == self.num_layers - 1:
                h = F.dropout(h, self.drop_ratio, training=self.training)
            else:
                h = F.dropout(F.relu(h), self.drop_ratio, training=self.training)
            if self.residual:
                h += h_list[layer]
            h_list.append(h)

        if self.JK == "last":
            node_representation = h_list[-1]
        elif self.JK == "sum":
            node_representation = 0
            for layer in range(self.num_layers + 1):
                node_representation += h_list[layer]
                
        return node_representation
```

## GIN's Graph Pooling Module

```python
class GINGraphPooling(nn.Module):

    def __init__(self, num_tasks=1, num_layers=5, emb_dim=300, residual=False, drop_ratio=0, JK="last", graph_pooling="sum"):
        """
        The module first embeds all the nodes, then pools the embeddings to obtain the graph embedding, and finally transforms the result linearly to the specified dimension.
        Args:
            num_tasks (int, optional): number of labels to be predicted. Defaults to 1 (dimension of graph representation).
            num_layers (int, optional): number of GINConv layers. Defaults to 5.
            emb_dim (int, optional): dimension of node embedding. Defaults to 300.
            residual (bool, optional): adding residual connection or not. Defaults to False.
            drop_ratio (float, optional): dropout rate. Defaults to 0.
            JK (str, optional): options are "last" & "sum". With "last", only node embeddings from the final layer would be considered; With "sum", the result would be the addition of node embeddings of all layers; Defaults to "last".
            graph_pooling (str, optional): pooling method of node embedding. Options are "sum", "mean", "max", "attention" and "set2set". Defaults to "sum".
        """
        super(GINGraphPooling, self).__init__()

        self.num_layers = num_layers
        self.drop_ratio = drop_ratio
        self.JK = JK
        self.emb_dim = emb_dim
        self.num_tasks = num_tasks

        if self.num_layers < 2:
            raise ValueError("Number of GNN layers must be greater than 1.")

        self.gnn_node = GINNodeEmbedding(num_layers, emb_dim, JK=JK, drop_ratio=drop_ratio, residual=residual)

        # Pooling function to generate whole-graph embeddings
        if graph_pooling == "sum":
            self.pool = global_add_pool
        elif graph_pooling == "mean":
            self.pool = global_mean_pool
        elif graph_pooling == "max":
            self.pool = global_max_pool
        elif graph_pooling == "attention":
            self.pool = GlobalAttention(gate_nn=nn.Sequential(
                nn.Linear(emb_dim, emb_dim), nn.BatchNorm1d(emb_dim), nn.ReLU(), nn.Linear(emb_dim, 1)))
        elif graph_pooling == "set2set":
            self.pool = Set2Set(emb_dim, processing_steps=2)
        else:
            raise ValueError("Invalid graph pooling type.")

        if graph_pooling == "set2set":
            self.graph_pred_linear = nn.Linear(2 * self.emb_dim, self.num_tasks)
        else:
            self.graph_pred_linear = nn.Linear(self.emb_dim, self.num_tasks)

    def forward(self, batched_data):
        h_node = self.gnn_node(batched_data)

        h_graph = self.pool(h_node, batched_data.batch)
        output = self.graph_pred_linear(h_graph)

        if self.training:
            return output
        else:
            # At inference time, relu is applied to output to ensure positivity
            return torch.clamp(output, min=0, max=50)
```

## Test on PCQM4M-LSC

**Practical Relevance:** Density Functional Theory (DFT) is a powerful and widely-used quantum physics calculation that can accurately predict various molecular properties such as the shape of molecules, reactivity, responses by electromagnetic fields. However, DFT is time-consuming and takes up to several hours per small molecule. Using fast and accurate ML models to approximate DFT enables diverse downstream applications, such as property prediction for organic photovoltaic devices and structure-based virtual screening for drug discovery.

**Overview:** PCQM4M-LSC is a quantum chemistry dataset originally curated under the PubChemQC project [1]. Based on the PubChemQC, we define a meaningful ML task of predicting DFT-calculated HOMO-LUMO energy gap of molecules given their 2D molecular graphs. The HOMO-LUMO gap is one of the most practically-relevant quantum chemical properties of molecules since it is related to reactivity, photoexcitation, and charge transport. Moreover, predicting the quantum chemical property only from 2D molecular graphs without their 3D equilibrium structures is also practically favorable. This is because obtaining 3D equilibrium structures requires DFT-based geometry optimization, which is expensive on its own.

> View more about the dataset in OGB (Open Graph Benchmark, Stanford)'s [website](https://ogb.stanford.edu/kddcup2021/pcqm4m/).

**Parameters (& Hyperparameters)**

| No. GINConv Layers | Graph Pooling Method | Embedding Dimension | Dropout Rate | Batch Size | Weight Decay |
| ------------------ | -------------------- | ------------------- | ------------ | ---------- | ------------ |
| 5                  | sum                  | 256                 | 0            | 512        | 1e5          |

**MAE Loss After 49 Epochs**

![](/assets/img/gin-loss.png)

- Download the code & output for the experiment [here](https://wwwcielwww.github.io/files/GIN.zip)

