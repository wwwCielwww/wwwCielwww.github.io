---
title: 'Study Notes: GNN Part (IV) ðŸŒ² Node & Link Prediction'
date: 2021-06-26
permalink: /posts/2021/06/gnn-4/
categories:
  - Computer Science
tags:
  - GNN
---

Generalize the model for node classification. Introduce the task of link prediction and its approach through building an encoder-decoder model with both positive and negative samples.

- View the code for node classification at [Google Colab](https://colab.research.google.com/drive/1flO6eG87ltGddO9ewnGYCzt4fs1Duq6T?usp=sharing) / Download the .ipynb file [here](https://wwwCielwww.github.io/files/Link_Prediction.ipynb)
- View the code for link prediction at [Google Colab](https://colab.research.google.com/drive/1U5ExgsaS4steyVpPYx2_GVAewVsBCnsv?usp=sharing) / Download the .ipynb file [here](https://wwwCielwww.github.io/files/Node_Classification_Generalized.ipynb)

## Node Classification - A Revision

Following from the last post of the series, for node classification task, we can use other convolutional layers (other than `GCNConv` & `GATConv`, documentation available [here](https://pytorch-geometric.readthedocs.io/en/latest/modules/nn.html#convolutional-layers)) or a different design of the network (e.g., number of layers, number of filters). An example of GNN using `SAGEConv` with number of filters for each layer specified by the parameter `hidden_channels` is provided below.

```python
import torch.nn.functional as F
from torch.nn import ReLU, Linear, Module
from torch_geometric.nn import SAGEConv, Sequential

class GraphSAGE(Module):
    def __init__(self, num_features, hidden_channels, num_classes):
        super().__init__()
        channels = [num_features] + hidden_channels
        convs = []
        for i in range(len(channels) - 1):
            convs.append((
                SAGEConv(channels[i], channels[i + 1]),
                "x, edge_index -> x"
            ))
            convs.append(ReLU(inplace=True))
        convs = convs[:-1]
        self.convs = Sequential("x, edge_index", convs)
        self.linear = Linear(channels[-1], num_classes)
        
    def forward(self, x, edge_index):
        x = self.convs(x, edge_index)
        x = F.dropout(x, p=0.5)
        return self.linear(x)
```

Notice that the `Sequential` here is an extension of the `torch.nn.Sequential` container in order to define a sequential GNN model. Since GNN operators take in multiple input arguments, `torch_geometric.nn.Sequential` expects both global input arguments, and function header definitions of individual operators. Here, where `'x, edge_index'` defines the input arguments of `convs`, and `'x, edge_index -> x'` defines the function header, *i.e.* input arguments *and* return types, of `SAGEConv`. For its more of its usage, see the [doc](https://pytorch-geometric.readthedocs.io/en/latest/modules/nn.html#torch_geometric.nn.sequential.Sequential).

## Link Prediction

- Task: predict if there exists an edge between a pair of nodes
- `edge_index` obtained from the dataset concerned includes all the possible pairs of nodes with a link. To train a model, we also need some node pairs without a link. 
- As a common practice, the data will be divided into those for training, evaluating and testing.

Fortunately, the function `train_test_split_edges(data, val_ratio=0.05, test_ratio=0.1)` in `PyG` can help sample node pairs without a link and divide the data (which includes both node pairs with and without a link). After applying the function, our data would have the attributes `train_pos_edge_index`, `train_neg_adj_mask`, `val_pos_edge_index`, `val_neg_edge_index`, `test_pos_edge_index` and `test_neg_edge_index`, substituting the original `edge_index`.

```python
from torch_geometric.utils import negative_sampling
from torch_geometric.datasets import Planetoid
from torch_geometric.transforms import NormalizeFeatures()
from torch_geometric.utils import train_test_split_edges

dataset = Planetoid('dataset', 'Cora', transform=NormalizeFeatures())
data = dataset[0]

# we're no longer interested in these
data.train_mask = data.val_mask = data.test_mask = data.y = None 

print(data.edge_index.shape)
# torch.Size([2, 10556])
print(data.x.shape)
# torch.Size([2708, 1433])

data = train_test_split_edges(data)

for key in data.keys:
    print(key, getattr(data, key).shape)
```

| `train_pos_edge_index` | `train_neg_adj_mask` | `val_pos_edge_index` | `val_neg_edge_index` | `test_pos_edge_index` | `test_neg_edge_index` |
| ---------------------- | -------------------- | -------------------- | -------------------- | --------------------- | --------------------- |
| [2, 8976]              | [2708, 2708]         | [2, 263]             | [2, 263]             | [2, 527]              | [2, 527]              |

Notice that `train_neg_adj_mask` is distinctly different from the other edge indices in terms of the shape, also that positive and negative samples of the same divided dataset have the same number of edges. 

For `Cora`, the graph is undirected, which means that the `edge_index` includes ($10556/2$) edges twice. Each two edges only differ in direction. When training a model, we need to consider information from both sides of an edge, which is not the case when evaluating or testing $\to8976/2+263+527=10556/2$.

Below we construct an encoder-decoder model to first learn the node embeddings and then generate the odds of a link existing between two nodes included in `edge_index`. The method `decode_all` can be used for inference: it will generate the odds of a link for all pairs of nodes in the graph.

```python
import torch
from torch.nn import ReLU, Module
from torch_geometric.nn import GCNConv, Sequential

class Net(Module):
    def __init__(self, channels):
        super().__init__()
        convs = []
        for i in range(len(channels) - 1):
            convs.append((
                GCNConv(channels[i], channels[i + 1]),
                "x, edge_index -> x"
            ))
            convs.append(ReLU(inplace=True))
        convs = convs[:-1]
        self.convs = Sequential("x, edge_index", convs)

    def encode(self, x, edge_index):
        return self.convs(x, edge_index)

    def decode(self, x, pos_edge_index, neg_edge_index):
        edge_index = torch.cat([pos_edge_index, neg_edge_index], dim=-1)
        return (x[edge_index[0]] * x[edge_index[1]]).sum(dim=-1)

    def decode_all(self, x):
        prob_adj = x @ x.t()
        return (prob_adj > 0).nonzero(as_tuple=False).t()
```

Define functions for training & testing:

```python
import torch.nn.functional as F
from torch_geometric.utils import negative_sampling
from sklearn.metrics import roc_auc_score

def get_link_labels(pos_edge_index, neg_edge_index):
    num_links = pos_edge_index.size(1) + neg_edge_index.size(1)
    link_labels = torch.zeros(num_links, dtype=torch.float)
    link_labels[:pos_edge_index.size(1)] = 1.
    return link_labels

def train(data, model, optimizer):
    model.train()

    neg_edge_index = negative_sampling(
        edge_index=data.train_pos_edge_index,
        num_nodes=data.num_nodes,
        num_neg_samples=data.train_pos_edge_index.size(1)
    )

    optimizer.zero_grad()
    x = model.encode(data.x, data.train_pos_edge_index)
    link_logits = model.decode(x, data.train_pos_edge_index, neg_edge_index)
    link_labels = get_link_labels(data.train_pos_edge_index, neg_edge_index).to(data.x.device)
    loss = F.binary_cross_entropy_with_logits(link_logits, link_labels)
    loss.backward()
    optimizer.step()

    return loss

@torch.no_grad()
def test(data, model):
    model.eval()

    x = model.encode(data.x, data.train_pos_edge_index)

    results = []
    for prefix in ["val", "test"]:
        pos_edge_index = data[f"{prefix}_pos_edge_index"]
        neg_edge_index = data[f"{prefix}_neg_edge_index"]
        link_logits = model.decode(x, pos_edge_index, neg_edge_index)
        link_probs = link_logits.sigmoid()
        link_labels = get_link_labels(pos_edge_index, neg_edge_index)
        results.append(roc_auc_score(link_labels.cpu(), link_probs.cpu()))

    return results
```

After 100 epochs, binary cross entropy of link logits and labels reaches 0.4407, `roc_auc_score` on validation and test datasets reach 0.9039 and 0.9374 respectively. ROC stands for curves receiver or operating characteristic curve. It illustrates in a binary classifier system the discrimination threshold created by plotting the true positive rate vs false positive rate. The `roc_auc_score` always runs from 0 to 1, and is sorting predictive possibilities.

### Reflection: Can we do this?

Consider the following code snippet.

```python
neg_edge_index = negative_sampling(
    edge_index=data.train_pos_edge_index,
    num_nodes=data.num_nodes,
    num_neg_samples=data.train_pos_edge_index.size(1))
```

One cannot help but wonder why limiting the edge index of positive samples to that of the training dataset can guarantee that the return value is of "truly" negative samples. I.e., it may as well include `val_pos_edge_index` or `test_pos_edge_index`. 

As far as I'm concerned, the training dataset is all that we've fed into our model. That is, the node embeddings were learnt merely based on `train_pos_edge_index` and `x`. Therefore, this simplification is made to ensure the loss converges.

