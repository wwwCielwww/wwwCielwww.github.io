{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Link Prediction.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "A46VO7zGgrFZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a17a8ea1-826b-4f3a-a97e-a47337038d7d"
      },
      "source": [
        "!pip install -q torch-scatter -f https://pytorch-geometric.com/whl/torch-1.9.0+cu102.html\n",
        "!pip install -q torch-sparse -f https://pytorch-geometric.com/whl/torch-1.9.0+cu102.html\n",
        "!pip install -q torch-cluster -f https://pytorch-geometric.com/whl/torch-1.9.0+cu102.html\n",
        "!pip install -q torch-spline-conv -f https://pytorch-geometric.com/whl/torch-1.9.0+cu102.html\n",
        "!pip install -q torch-geometric"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[K     |████████████████████████████████| 2.6MB 7.4MB/s \n",
            "\u001b[K     |████████████████████████████████| 1.4MB 8.1MB/s \n",
            "\u001b[K     |████████████████████████████████| 931kB 7.2MB/s \n",
            "\u001b[K     |████████████████████████████████| 389kB 309kB/s \n",
            "\u001b[K     |████████████████████████████████| 225kB 8.4MB/s \n",
            "\u001b[K     |████████████████████████████████| 235kB 48.1MB/s \n",
            "\u001b[K     |████████████████████████████████| 51kB 8.6MB/s \n",
            "\u001b[?25h  Building wheel for torch-geometric (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GlR7DTuTgzUY"
      },
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from torch.nn import ReLU, Module\n",
        "from torch_geometric.nn import GCNConv, Sequential\n",
        "from torch_geometric.datasets import Planetoid\n",
        "from torch_geometric.utils import train_test_split_edges, negative_sampling\n",
        "from torch_geometric.transforms import NormalizeFeatures\n",
        "from sklearn.metrics import roc_auc_score\n",
        "\n",
        "\n",
        "class Net(Module):\n",
        "    def __init__(self, channels):\n",
        "        super().__init__()\n",
        "        convs = []\n",
        "        for i in range(len(channels) - 1):\n",
        "            convs.append((\n",
        "                GCNConv(channels[i], channels[i + 1]),\n",
        "                \"x, edge_index -> x\"\n",
        "            ))\n",
        "            convs.append(ReLU())\n",
        "        convs = convs[:-1]\n",
        "        self.convs = Sequential(\"x, edge_index\", convs)\n",
        "\n",
        "    def encode(self, x, edge_index):\n",
        "        return self.convs(x, edge_index)\n",
        "\n",
        "    def decode(self, x, pos_edge_index, neg_edge_index):\n",
        "        edge_index = torch.cat([pos_edge_index, neg_edge_index], dim=-1)\n",
        "        return (x[edge_index[0]] * x[edge_index[1]]).sum(dim=-1)\n",
        "\n",
        "    def decode_all(self, x):\n",
        "        prob_adj = x @ x.t()\n",
        "        return (prob_adj > 0).nonzero(as_tuple=False).t()"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XHoNj-q6jVLU"
      },
      "source": [
        "def get_link_labels(pos_edge_index, neg_edge_index):\n",
        "    num_links = pos_edge_index.size(1) + neg_edge_index.size(1)\n",
        "    link_labels = torch.zeros(num_links, dtype=torch.float)\n",
        "    link_labels[:pos_edge_index.size(1)] = 1.\n",
        "    return link_labels\n",
        "\n",
        "def train(data, model, optimizer):\n",
        "    model.train()\n",
        "\n",
        "    neg_edge_index = negative_sampling(\n",
        "        edge_index=data.train_pos_edge_index,\n",
        "        num_nodes=data.num_nodes,\n",
        "        num_neg_samples=data.train_pos_edge_index.size(1)\n",
        "    )\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "    x = model.encode(data.x, data.train_pos_edge_index)\n",
        "    link_logits = model.decode(x, data.train_pos_edge_index, neg_edge_index)\n",
        "    link_labels = get_link_labels(data.train_pos_edge_index, neg_edge_index).to(data.x.device)\n",
        "    loss = F.binary_cross_entropy_with_logits(link_logits, link_labels)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    return loss\n",
        "\n",
        "@torch.no_grad()\n",
        "def test(data, model):\n",
        "    model.eval()\n",
        "\n",
        "    x = model.encode(data.x, data.train_pos_edge_index)\n",
        "\n",
        "    results = []\n",
        "    for prefix in [\"val\", \"test\"]:\n",
        "        pos_edge_index = data[f\"{prefix}_pos_edge_index\"]\n",
        "        neg_edge_index = data[f\"{prefix}_neg_edge_index\"]\n",
        "        link_logits = model.decode(x, pos_edge_index, neg_edge_index)\n",
        "        link_probs = link_logits.sigmoid()\n",
        "        link_labels = get_link_labels(pos_edge_index, neg_edge_index)\n",
        "        results.append(roc_auc_score(link_labels.cpu(), link_probs.cpu()))\n",
        "\n",
        "    return results"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BMLZDqBCOpXJ",
        "outputId": "c2b4425b-de55-460d-c2f6-fba3c6856857"
      },
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "dataset = Planetoid(\"data\", \"Cora\", transform=NormalizeFeatures())\n",
        "data = dataset[0]\n",
        "\n",
        "data.train_mask = data.val_mask = data.test_mask = data.y = None\n",
        "data = train_test_split_edges(data)\n",
        "data.to(device)\n",
        "\n",
        "model = Net([dataset.num_features, 128, 64]).to(device)\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
        "\n",
        "best_val_auc = test_auc = 0\n",
        "for epoch in range(100):\n",
        "    loss = train(data, model, optimizer)\n",
        "    val_auc, tmp_test_auc = test(data, model)\n",
        "    if val_auc > best_val_auc:\n",
        "        best_val_auc = val_auc\n",
        "        test_auc = tmp_test_auc\n",
        "    print(f\"Epoch {epoch + 1} Loss {loss:.4f}\\nVal Auc {val_auc} Test Auc {test_auc}\")\n",
        "\n",
        "x = model.encode(data.x, data.train_pos_edge_index)\n",
        "final_edge_index = model.decode_all(x)"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1 Loss 0.6930\n",
            "Val Auc 0.6925212161517442 Test Auc 0.6961066363253388\n",
            "Epoch 2 Loss 0.6807\n",
            "Val Auc 0.6894707166505225 Test Auc 0.6961066363253388\n",
            "Epoch 3 Loss 0.7151\n",
            "Val Auc 0.6897887782098917 Test Auc 0.6961066363253388\n",
            "Epoch 4 Loss 0.6766\n",
            "Val Auc 0.7045208113461232 Test Auc 0.7155752550147806\n",
            "Epoch 5 Loss 0.6848\n",
            "Val Auc 0.7396593849846029 Test Auc 0.760855006139078\n",
            "Epoch 6 Loss 0.6891\n",
            "Val Auc 0.7714004828752766 Test Auc 0.7903909926583108\n",
            "Epoch 7 Loss 0.6906\n",
            "Val Auc 0.7354884413537857 Test Auc 0.7903909926583108\n",
            "Epoch 8 Loss 0.6906\n",
            "Val Auc 0.7075279388165218 Test Auc 0.7903909926583108\n",
            "Epoch 9 Loss 0.6895\n",
            "Val Auc 0.697407798291142 Test Auc 0.7903909926583108\n",
            "Epoch 10 Loss 0.6870\n",
            "Val Auc 0.6940103225433361 Test Auc 0.7903909926583108\n",
            "Epoch 11 Loss 0.6830\n",
            "Val Auc 0.6925645881825673 Test Auc 0.7903909926583108\n",
            "Epoch 12 Loss 0.6798\n",
            "Val Auc 0.7025401552718704 Test Auc 0.7903909926583108\n",
            "Epoch 13 Loss 0.6793\n",
            "Val Auc 0.7138024259422573 Test Auc 0.7903909926583108\n",
            "Epoch 14 Loss 0.6742\n",
            "Val Auc 0.7188046668305166 Test Auc 0.7903909926583108\n",
            "Epoch 15 Loss 0.6678\n",
            "Val Auc 0.7151903309285952 Test Auc 0.7903909926583108\n",
            "Epoch 16 Loss 0.6623\n",
            "Val Auc 0.70641472335873 Test Auc 0.7903909926583108\n",
            "Epoch 17 Loss 0.6563\n",
            "Val Auc 0.7020196909019937 Test Auc 0.7903909926583108\n",
            "Epoch 18 Loss 0.6481\n",
            "Val Auc 0.7062556925790456 Test Auc 0.7903909926583108\n",
            "Epoch 19 Loss 0.6378\n",
            "Val Auc 0.7208431522792002 Test Auc 0.7903909926583108\n",
            "Epoch 20 Loss 0.6257\n",
            "Val Auc 0.7532276019604158 Test Auc 0.7903909926583108\n",
            "Epoch 21 Loss 0.6107\n",
            "Val Auc 0.7797712848241265 Test Auc 0.8157700492206432\n",
            "Epoch 22 Loss 0.5976\n",
            "Val Auc 0.7881565441165841 Test Auc 0.8175703653561566\n",
            "Epoch 23 Loss 0.5794\n",
            "Val Auc 0.7832699619771863 Test Auc 0.8175703653561566\n",
            "Epoch 24 Loss 0.5658\n",
            "Val Auc 0.7847590683687778 Test Auc 0.8175703653561566\n",
            "Epoch 25 Loss 0.5571\n",
            "Val Auc 0.7933756451589586 Test Auc 0.8092385022810005\n",
            "Epoch 26 Loss 0.5505\n",
            "Val Auc 0.795327386545996 Test Auc 0.8103582989172898\n",
            "Epoch 27 Loss 0.5450\n",
            "Val Auc 0.7973369573074643 Test Auc 0.8165873927461663\n",
            "Epoch 28 Loss 0.5394\n",
            "Val Auc 0.8075727565817057 Test Auc 0.8278285666963119\n",
            "Epoch 29 Loss 0.5320\n",
            "Val Auc 0.8167965418034091 Test Auc 0.8371289998523741\n",
            "Epoch 30 Loss 0.5281\n",
            "Val Auc 0.8230131995547139 Test Auc 0.8443014593362594\n",
            "Epoch 31 Loss 0.5159\n",
            "Val Auc 0.8271624571701196 Test Auc 0.847181965153081\n",
            "Epoch 32 Loss 0.5144\n",
            "Val Auc 0.8316153190012868 Test Auc 0.8498068260786594\n",
            "Epoch 33 Loss 0.5104\n",
            "Val Auc 0.8337116338244009 Test Auc 0.8513911042779112\n",
            "Epoch 34 Loss 0.5106\n",
            "Val Auc 0.8355621738061849 Test Auc 0.850843808172715\n",
            "Epoch 35 Loss 0.5126\n",
            "Val Auc 0.8369645361361303 Test Auc 0.8491515110053326\n",
            "Epoch 36 Loss 0.5132\n",
            "Val Auc 0.8408246468793825 Test Auc 0.8527521432763593\n",
            "Epoch 37 Loss 0.5035\n",
            "Val Auc 0.8445690988737728 Test Auc 0.8590064415311329\n",
            "Epoch 38 Loss 0.4987\n",
            "Val Auc 0.8486460697711402 Test Auc 0.8635468388248976\n",
            "Epoch 39 Loss 0.5022\n",
            "Val Auc 0.8530844742586997 Test Auc 0.8678351918596906\n",
            "Epoch 40 Loss 0.4931\n",
            "Val Auc 0.8579276843672743 Test Auc 0.8737042224614642\n",
            "Epoch 41 Loss 0.4880\n",
            "Val Auc 0.861990197921034 Test Auc 0.8787415070086306\n",
            "Epoch 42 Loss 0.4866\n",
            "Val Auc 0.8652864722635862 Test Auc 0.8833143099928348\n",
            "Epoch 43 Loss 0.4861\n",
            "Val Auc 0.868105654267085 Test Auc 0.8867853195021046\n",
            "Epoch 44 Loss 0.4812\n",
            "Val Auc 0.8708670068961529 Test Auc 0.8905659833866826\n",
            "Epoch 45 Loss 0.4832\n",
            "Val Auc 0.8762017666873888 Test Auc 0.8935833132298032\n",
            "Epoch 46 Loss 0.4810\n",
            "Val Auc 0.8792667235322182 Test Auc 0.8973747790111943\n",
            "Epoch 47 Loss 0.4778\n",
            "Val Auc 0.8798594746201331 Test Auc 0.9001616683889693\n",
            "Epoch 48 Loss 0.4824\n",
            "Val Auc 0.8810449767959636 Test Auc 0.9020880066539683\n",
            "Epoch 49 Loss 0.4750\n",
            "Val Auc 0.8819124174124245 Test Auc 0.9030277716767064\n",
            "Epoch 50 Loss 0.4725\n",
            "Val Auc 0.8828376874033166 Test Auc 0.9039711373317153\n",
            "Epoch 51 Loss 0.4732\n",
            "Val Auc 0.8832714077115471 Test Auc 0.9057858559963129\n",
            "Epoch 52 Loss 0.4756\n",
            "Val Auc 0.8827364859980628 Test Auc 0.9057858559963129\n",
            "Epoch 53 Loss 0.4739\n",
            "Val Auc 0.8823750524078705 Test Auc 0.9057858559963129\n",
            "Epoch 54 Loss 0.4705\n",
            "Val Auc 0.88350272520927 Test Auc 0.9064627748632659\n",
            "Epoch 55 Loss 0.4691\n",
            "Val Auc 0.8852665211294077 Test Auc 0.907665386041789\n",
            "Epoch 56 Loss 0.4689\n",
            "Val Auc 0.8863652792435918 Test Auc 0.9091704503310781\n",
            "Epoch 57 Loss 0.4668\n",
            "Val Auc 0.8861628764330842 Test Auc 0.9091704503310781\n",
            "Epoch 58 Loss 0.4627\n",
            "Val Auc 0.8865965967413147 Test Auc 0.9102830457028254\n",
            "Epoch 59 Loss 0.4676\n",
            "Val Auc 0.8879845017276525 Test Auc 0.9113956410745726\n",
            "Epoch 60 Loss 0.4684\n",
            "Val Auc 0.8897627549913977 Test Auc 0.9117268992435071\n",
            "Epoch 61 Loss 0.4659\n",
            "Val Auc 0.8906446529514668 Test Auc 0.9130915388742266\n",
            "Epoch 62 Loss 0.4678\n",
            "Val Auc 0.8912807760702048 Test Auc 0.9150286790360387\n",
            "Epoch 63 Loss 0.4635\n",
            "Val Auc 0.8914542641934972 Test Auc 0.9163753155054029\n",
            "Epoch 64 Loss 0.4633\n",
            "Val Auc 0.8938975552631959 Test Auc 0.9179703956014676\n",
            "Epoch 65 Loss 0.4571\n",
            "Val Auc 0.8947505385360494 Test Auc 0.9181468265827479\n",
            "Epoch 66 Loss 0.4629\n",
            "Val Auc 0.895545692434472 Test Auc 0.9187985410238038\n",
            "Epoch 67 Loss 0.4570\n",
            "Val Auc 0.8962251875840334 Test Auc 0.9217294556924195\n",
            "Epoch 68 Loss 0.4588\n",
            "Val Auc 0.8967456519539101 Test Auc 0.9226296137601763\n",
            "Epoch 69 Loss 0.4552\n",
            "Val Auc 0.8973962324162559 Test Auc 0.9232165168203535\n",
            "Epoch 70 Loss 0.4577\n",
            "Val Auc 0.899319059116078 Test Auc 0.9241886875335309\n",
            "Epoch 71 Loss 0.4536\n",
            "Val Auc 0.8998973528603855 Test Auc 0.9241958887980729\n",
            "Epoch 72 Loss 0.4543\n",
            "Val Auc 0.900099755670893 Test Auc 0.924750386167811\n",
            "Epoch 73 Loss 0.4518\n",
            "Val Auc 0.899405803177724 Test Auc 0.924750386167811\n",
            "Epoch 74 Loss 0.4551\n",
            "Val Auc 0.899781694111524 Test Auc 0.924750386167811\n",
            "Epoch 75 Loss 0.4502\n",
            "Val Auc 0.8999118102039931 Test Auc 0.924750386167811\n",
            "Epoch 76 Loss 0.4525\n",
            "Val Auc 0.8997672367679161 Test Auc 0.924750386167811\n",
            "Epoch 77 Loss 0.4531\n",
            "Val Auc 0.8994925472393704 Test Auc 0.924750386167811\n",
            "Epoch 78 Loss 0.4471\n",
            "Val Auc 0.899044369587532 Test Auc 0.924750386167811\n",
            "Epoch 79 Loss 0.4488\n",
            "Val Auc 0.8997816941115239 Test Auc 0.924750386167811\n",
            "Epoch 80 Loss 0.4525\n",
            "Val Auc 0.900287701137793 Test Auc 0.9281601849284735\n",
            "Epoch 81 Loss 0.4485\n",
            "Val Auc 0.9011985137850771 Test Auc 0.9290747455253142\n",
            "Epoch 82 Loss 0.4494\n",
            "Val Auc 0.9014876606572308 Test Auc 0.9293699973715384\n",
            "Epoch 83 Loss 0.4509\n",
            "Val Auc 0.901747892842169 Test Auc 0.9292403746097816\n",
            "Epoch 84 Loss 0.4478\n",
            "Val Auc 0.9010539403490002 Test Auc 0.9292403746097816\n",
            "Epoch 85 Loss 0.4473\n",
            "Val Auc 0.9016611487805231 Test Auc 0.9292403746097816\n",
            "Epoch 86 Loss 0.4476\n",
            "Val Auc 0.9029044803307841 Test Auc 0.9314943704114442\n",
            "Epoch 87 Loss 0.4477\n",
            "Val Auc 0.9036128901675606 Test Auc 0.9323189152015092\n",
            "Epoch 88 Loss 0.4516\n",
            "Val Auc 0.9018780089346383 Test Auc 0.9323189152015092\n",
            "Epoch 89 Loss 0.4530\n",
            "Val Auc 0.9017478928421692 Test Auc 0.9323189152015092\n",
            "Epoch 90 Loss 0.4489\n",
            "Val Auc 0.9019358383090691 Test Auc 0.9323189152015092\n",
            "Epoch 91 Loss 0.4449\n",
            "Val Auc 0.9023984733045151 Test Auc 0.9323189152015092\n",
            "Epoch 92 Loss 0.4420\n",
            "Val Auc 0.9028032789255303 Test Auc 0.9323189152015092\n",
            "Epoch 93 Loss 0.4448\n",
            "Val Auc 0.9012997151903309 Test Auc 0.9323189152015092\n",
            "Epoch 94 Loss 0.4433\n",
            "Val Auc 0.9015599473752693 Test Auc 0.9323189152015092\n",
            "Epoch 95 Loss 0.4454\n",
            "Val Auc 0.9030345964232531 Test Auc 0.9323189152015092\n",
            "Epoch 96 Loss 0.4462\n",
            "Val Auc 0.9029333950179994 Test Auc 0.9323189152015092\n",
            "Epoch 97 Loss 0.4426\n",
            "Val Auc 0.9026876201766687 Test Auc 0.9323189152015092\n",
            "Epoch 98 Loss 0.4415\n",
            "Val Auc 0.9020804117451459 Test Auc 0.9323189152015092\n",
            "Epoch 99 Loss 0.4470\n",
            "Val Auc 0.9027599068947072 Test Auc 0.9323189152015092\n",
            "Epoch 100 Loss 0.4407\n",
            "Val Auc 0.9039309517269298 Test Auc 0.9373670016454889\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VPftZox6RjQr",
        "outputId": "cd85e472-9aa8-4200-ac4c-e2fa39d9ca17"
      },
      "source": [
        "(x, x.shape), (final_edge_index, final_edge_index.shape), best_val_auc"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((tensor([[ 0.1374, -0.1213, -0.0143,  ..., -0.2595,  0.0808, -0.1902],\n",
              "          [ 0.0888, -0.0660,  0.3673,  ..., -0.0973, -0.1418, -0.1412],\n",
              "          [ 0.1031, -0.0878,  0.2762,  ..., -0.2299, -0.1008, -0.2051],\n",
              "          ...,\n",
              "          [ 0.0132, -0.0505, -0.3180,  ..., -0.0432, -0.1375, -0.1339],\n",
              "          [ 0.1229, -0.1517,  0.0348,  ..., -0.3297, -0.1083, -0.2379],\n",
              "          [ 0.1277, -0.1434, -0.0026,  ..., -0.3051, -0.0228, -0.2208]],\n",
              "         device='cuda:0', grad_fn=<AddBackward0>), torch.Size([2708, 64])),\n",
              " (tensor([[   0,    0,    0,  ..., 2707, 2707, 2707],\n",
              "          [   0,    1,    2,  ..., 2705, 2706, 2707]], device='cuda:0'),\n",
              "  torch.Size([2, 3394548])),\n",
              " 0.9039309517269298)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    }
  ]
}